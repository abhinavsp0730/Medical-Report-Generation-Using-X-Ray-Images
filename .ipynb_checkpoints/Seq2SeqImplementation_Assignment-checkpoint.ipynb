{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DKpp1EzDQXDt"
   },
   "source": [
    "<pre>\n",
    "1. Download the Italian to English translation dataset from <a href=\"http://www.manythings.org/anki/ita-eng.zip\">here</a>\n",
    "\n",
    "2. You will find ita.txt file in that ZIP, you can read that data using python and preprocess that data. \n",
    "\n",
    "3. You have to implement an Encoder and Decoder architecture with Luong attention.\n",
    "\n",
    "Encoder   - with 1 layer LSTM \n",
    "Decoder   - with 1 layer LSTM\n",
    "attention - Luone attention. (Please refer the reference notebook to know more about the attention mechanisum.)\n",
    "\n",
    "4. In Global attention, we have 3 types of scoring functions(as discussed in the reference notebook). As a part of this assignment <strong>you need to create 3 models for each scoring function.</strong>\n",
    "<img src='https://i.imgur.com/iD2jZo3.png'>\n",
    "    In model 1 you need to implemnt \"dot\" score function\n",
    "    In model 2 you need to implemnt \"general\" score function\n",
    "    In model 3 you need to implemnt \"concat\" score function\n",
    "    \n",
    "\n",
    "    <strong>Please do add the markdown titles for each model  so that we can have a better look at the code and verify.</strong>\n",
    "\n",
    "5. Using attention weights, you can plot the attention plots, please plot those for 2-3 examples. You can check about those in <a href=\"https://www.tensorflow.org/tutorials/text/nmt_with_attention#translate\">this</a>\n",
    "\n",
    "6. The attention layer has to be written by yourself only. The main objective of this assignment is to read and implement a paper on yourself so please do it yourself.  \n",
    "\n",
    "7. You can use any tf.Keras highlevel API's to build and train the models. Check the reference notebook for better understanding.\n",
    "\n",
    "8. Use BLEU score as metric to evaluate your model. You can use any loss function you need.\n",
    "\n",
    "9. You have to use Tensorboard to plot the Graph, Scores and histograms of gradients. \n",
    "\n",
    "10. Resources:\n",
    "    a. Check the reference notebook\n",
    "    b. <a href=\"https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/\">Resource 1</a>\n",
    "    c. <a href=\"https://www.tensorflow.org/tutorials/text/nmt_with_attention\">Resource 2</a>\n",
    "    \n",
    "\n",
    "Note 1:  There are many blogs on the attention mechanisum which might be misleading you, so do read the references completly and after that only please check the internet. the best things is to read the research papers and try to implement it on your own. \n",
    "\n",
    "Note 2: To complete this assignment, the reference that are mentioned will be enough.\n",
    "\n",
    "Note 3: If you are starting this assignment, you might have completed minimum of 20 assignment. If  you are still not able to implement this algorithm you might have rushed in the previous assignments with out learning much and didn't spend your time productively.\n",
    "</pre>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Seq2SeqImplementation_Assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
