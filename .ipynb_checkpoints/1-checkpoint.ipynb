{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z0qDwzAqQaAP"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "#!pip install tf-nightly-2.0-preview\n",
    "import tensorflow as tf\n",
    "# tf.enable_eager_execution()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import functools\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ssOtz6N6Lr7q"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import xml.etree.ElementTree\n",
    "import numpy as np\n",
    "\n",
    "image_folder = ''\n",
    "annotation_folder = ''\n",
    "\n",
    "def maybe_download():\n",
    "    image_zip = 'NLMCXR_png.tgz'\n",
    "    if not os.path.exists(os.path.abspath('.') + '/dataset/' + image_zip):\n",
    "        tf.keras.utils.get_file('NLMCXR_png.tgz',\n",
    "                                cache_subdir=os.path.abspath('.') + '/dataset/',\n",
    "                                origin = 'https://openi.nlm.nih.gov/imgs/collections/NLMCXR_png.tgz',\n",
    "                                extract = True)\n",
    "        tf.keras.utils.get_file('NLMCXR_reports.tgz',\n",
    "                                cache_subdir=os.path.abspath('.') + '/dataset/',\n",
    "                                origin = 'https://openi.nlm.nih.gov/imgs/collections/NLMCXR_reports.tgz',\n",
    "                                extract = True)\n",
    "\n",
    "    global image_folder, annotation_folder\n",
    "    image_folder = './dataset/'\n",
    "    annotation_folder = './dataset/ecgen-radiology/'\n",
    "\n",
    "def extract_data():\n",
    "    all_findings = []\n",
    "    all_impressions = []\n",
    "    all_img_names = []\n",
    "    rids = []\n",
    "\n",
    "    total_count = 0 # Count of reports available in the dataset\n",
    "    no_image_count = 0 # Count of reports having no associated chest image\n",
    "    no_impression_count = 0 # Count of reports having an empty \"Impression\" section\n",
    "    no_findings_count = 0 # Count of reports having an empty \"Findings\" section\n",
    "\n",
    "    # Storing impressions, findings and the image names in vectors\n",
    "    for file in os.listdir(annotation_folder):\n",
    "        total_count += 1\n",
    "        file = os.path.abspath(annotation_folder) + '/' + file\n",
    "        e = xml.etree.ElementTree.parse(file).getroot()\n",
    "\n",
    "        rid = e.find('pmcId').get('id') # Report Id\n",
    "        # We choose to ignore reports having no associated image\n",
    "        image_id = e.find('parentImage')\n",
    "        if image_id is None:\n",
    "            no_image_count += 1\n",
    "            continue\n",
    "\n",
    "        image_id = image_id.get('id')\n",
    "        image_name = os.path.abspath('.') + '/' + image_id + '.png'\n",
    "        findings = ''\n",
    "        impression = ''\n",
    "\n",
    "        # Parsing \"Impression\" and \"Findings\"\n",
    "        for element in e.findall('MedlineCitation/Article/Abstract/AbstractText'):\n",
    "            if element.get('Label') == 'FINDINGS':\n",
    "                findings = element.text\n",
    "            if element.get('Label') == 'IMPRESSION':\n",
    "                impression = element.text\n",
    "\n",
    "        # Sanity check: Skip this report if it has an empty \"Impression\" section\n",
    "        if findings is None:\n",
    "            no_findings_count += 1\n",
    "            #findings = 'No finding'\n",
    "            continue\n",
    "        if impression is None:\n",
    "            no_impression_count += 1\n",
    "            continue\n",
    "\n",
    "        # Transforming findings and impressions into lists of sentences\n",
    "        findings = findings.replace(\"XXXX\", \"\") #\"XXXX\" represents information anonymized\n",
    "        sentences = findings.split('.')\n",
    "        del sentences[-1]\n",
    "        sentences = ['<start> ' + sentence + ' <end>' for sentence in sentences]\n",
    "        findings = sentences\n",
    "\n",
    "        impression = impression.replace(\"XXXX\", \"\") #\"XXXX\" represents information anonymized\n",
    "        sentences = impression.split('.')\n",
    "        del sentences[-1]\n",
    "        sentences = ['<start> ' + sentence + ' <end>' for sentence in sentences]\n",
    "        impression = sentences\n",
    "\n",
    "        #appending to vectors\n",
    "        all_img_names.append(image_name)\n",
    "        all_findings.append(findings)\n",
    "        all_impressions.append(impression)\n",
    "        rids.append(rid)\n",
    "\n",
    "    print(\"Number of reports available:\", total_count)\n",
    "    print(\"Number of reports selected:\", len(all_img_names))\n",
    "    print(\"Number of reports not having images (skipped):\", no_image_count)\n",
    "    print(\"Number of reports with Impression section empty (skipped):\", no_impression_count)\n",
    "    print(\"Number of reports with Findings section empty:\", no_findings_count)\n",
    "    print(\"Total skipped:\", no_image_count + no_impression_count + no_findings_count)\n",
    "\n",
    "    return all_findings, all_impressions, all_img_names, rids\n",
    "\n",
    "def init_inception_model():\n",
    "    # Initialize InceptionV3 and load the pretrained Imagenet weights\n",
    "    image_model = tf.keras.applications.InceptionV3(include_top=False,\n",
    "                                                    weights='imagenet')\n",
    "    new_input = image_model.input\n",
    "    hidden_layer = image_model.layers[-1].output\n",
    "\n",
    "    return tf.keras.Model(new_input, hidden_layer)\n",
    "\n",
    "def transform_input(all_findings, all_impressions, max_paragraph_length, max_sentence_length):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token=\"<unk>\",\n",
    "                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
    "\n",
    "    findings_texts = [' '.join(findings) for findings in all_findings]\n",
    "    impressions_texts = [' '.join(impression) for impression in all_impressions]\n",
    "    tokenizer.fit_on_texts(findings_texts + impressions_texts)\n",
    "    all_findings_seq = [tokenizer.texts_to_sequences(findings) for findings in all_findings]\n",
    "    all_impressions_seq = [tokenizer.texts_to_sequences(impression) for impression in all_impressions]\n",
    "\n",
    "    tokenizer.word_index['<pad>'] = 0\n",
    "\n",
    "    # Adding empty sentence seqs to each paragraph to have a fixed length for each\n",
    "    for findings in all_findings_seq:\n",
    "        while len(findings) < max_paragraph_length:\n",
    "            findings.append([0])\n",
    "        if len(findings) > max_paragraph_length:\n",
    "            del findings[max_paragraph_length:]\n",
    "\n",
    "    for impressions in all_impressions_seq:\n",
    "        while len(impressions) < max_paragraph_length:\n",
    "            impressions.append([0])\n",
    "        if len(impressions) > max_paragraph_length:\n",
    "            del impressions[max_paragraph_length:]\n",
    "\n",
    "    # Padding sequences\n",
    "    pad_sequences = tf.keras.preprocessing.sequence.pad_sequences\n",
    "    findings_vector = [pad_sequences(findings, padding='post', maxlen=max_sentence_length) for findings in all_findings_seq]\n",
    "    impressions_vector = [pad_sequences(impressions, padding='post', maxlen=max_sentence_length) for impressions in all_impressions_seq]\n",
    "\n",
    "    #print(findings_vector[0])\n",
    "    #print(impressions_vector[0])\n",
    "\n",
    "    # Combining findings and impressions\n",
    "    for i in range(len(findings_vector)):\n",
    "        findings_vector[i] = np.concatenate((findings_vector[i], impressions_vector[i]))\n",
    "    # Now, for a given batch \"i\", we can retrieve impressions = findings[i, max_paragraph_length:]\n",
    "    #print(findings_vector[0])\n",
    "\n",
    "    return tokenizer, findings_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yizF3X6QRJpy"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def gru(units):\n",
    "    return tf.keras.layers.GRU(units,\n",
    "                             return_sequences=True,\n",
    "                             return_state=True,\n",
    "                             recurrent_activation='sigmoid',\n",
    "                             recurrent_initializer='glorot_uniform')\n",
    "\n",
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, key, query):\n",
    "        # features(CNN_encoder output) shape: (batch_size, 64, embedding_dim)\n",
    "\n",
    "        #print(\"Key Shape:\", key.shape)\n",
    "        #print(\"Query Shape:\", query.shape)\n",
    "\n",
    "        score = tf.nn.tanh(self.W1(key) + self.W2(query))\n",
    "        attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
    "        context_vector = attention_weights*key\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "class CNN_Encoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(CNN_Encoder, self).__init__()\n",
    "        # initial shape: (batch_size, 64, 2048)\n",
    "        # shape after passing through fc: (batch_size, 64, embedding_dim)\n",
    "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x\n",
    "\n",
    "class Sentence_Encoder(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(Sentence_Encoder, self).__init__()\n",
    "        self.attention = BahdanauAttention(units)\n",
    "        self.fc = tf.keras.layers.Dense(units)\n",
    "\n",
    "    def call(self, hidden_states, features):\n",
    "        # hidden_states: (batch_size, max_sentence_length, units + units)\n",
    "        # features: (batch_size, 64, embedding_dim)\n",
    "        features = tf.expand_dims(features, 1)\n",
    "        features = tf.reshape(features, (features.shape[0], features.shape[1], -1))\n",
    "        # context_vector: (batch_size, units + units)\n",
    "        # word_weights: (batch_size, max_sentence_length)\n",
    "        context_vector, word_weights = self.attention(hidden_states, features)\n",
    "        # encoded_sentence: (batch_size, units)\n",
    "        encoded_sentence = self.fc(context_vector)\n",
    "\n",
    "        return encoded_sentence, word_weights\n",
    "\n",
    "class Paragraph_Encoder(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(Paragraph_Encoder, self).__init__()\n",
    "        self.attention = BahdanauAttention(units)\n",
    "\n",
    "    def call(self, encoded_sentences, features):\n",
    "        # encoded_sentences: (batch_size, MAX_PARAGRAPH_LENGTH, units)\n",
    "        # features: (batch_size, 64, embedding_dim)\n",
    "        features = tf.expand_dims(features, 1)\n",
    "        features = tf.reshape(features, (features.shape[0], features.shape[1], -1))\n",
    "        # encoded_paragraph: (batch_size, units)\n",
    "        # sentence_weights: (batch_size, MAX_PARAGRAPH_LENGTH)\n",
    "        encoded_paragraph, sentence_weights = self.attention(encoded_sentences, features)\n",
    "        return encoded_paragraph, sentence_weights\n",
    "\n",
    "class Word_Decoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size):\n",
    "        super(Word_Decoder, self).__init__()\n",
    "\n",
    "        self.attention = BahdanauAttention(units)\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(units)\n",
    "        self.fc1 = tf.keras.layers.Dense(units)\n",
    "        self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, x, features, prev_sentence, hidden):\n",
    "        # x: (batch_size, 1)\n",
    "        # features: (batch_size, 64, embedding_dim)\n",
    "        # prev_sentence: (batch_size, units)\n",
    "        # hidden: (batch_size, units)\n",
    "\n",
    "        # visual_context: (batch_size, embedding)\n",
    "        # visual_weights: (batch_size, 64)\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        visual_context, visual_weights = self.attention(features, hidden_with_time_axis)\n",
    "\n",
    "        # x shape after passing through embedding: (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation:(batch_size, 1, embedding_dim + embedding_dim + units)\n",
    "        x = tf.concat([tf.expand_dims(visual_context, 1), tf.expand_dims(prev_sentence, 1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        # output: (batch_size, 1, units)\n",
    "        output, state = self.gru(x)\n",
    "        # shape: (batch_size, 1, units)\n",
    "        x = self.fc1(output)\n",
    "        # x shape: (batch_size * 1, units)\n",
    "        x = tf.reshape(x, (-1, x.shape[2]))\n",
    "        # output shape: (batch_size * 1, vocab_size)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x, state, visual_weights\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self, tokenizer, embedding_dim, units):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.units = units\n",
    "\n",
    "        self.image_encoder = CNN_Encoder(embedding_dim)\n",
    "        self.sentence_encoder = Sentence_Encoder(units)\n",
    "        self.paragraph_encoder = Paragraph_Encoder(units)\n",
    "        self.fwd_decoder = Word_Decoder(embedding_dim, units, len(tokenizer.word_index))\n",
    "        self.bwd_decoder = Word_Decoder(embedding_dim, units, len(tokenizer.word_index))\n",
    "\n",
    "    def loss_function(self, real, pred):\n",
    "        mask = 1 - np.equal(real, 0)\n",
    "        loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
    "        return tf.reduce_mean(loss_)\n",
    "\n",
    "    def tensors_are_same(self, a, b):\n",
    "        r = str(tf.reduce_all(tf.equal(a, b))) # In a perfect world, I would just compare tf.reduce_all(tf.equal(a, b)).numpy()\n",
    "        return r[10] == 'T'\n",
    "\n",
    "    def train_word_decoder(self, batch_size, loss, features, findings, i, \\\n",
    "                           prev_sentence, fwd_hidden, bwd_hidden):\n",
    "        is_training_impressions = (i >= int(findings.shape[1]/2))\n",
    "\n",
    "        fwd_input = tf.expand_dims([self.tokenizer.word_index['<start>']] * batch_size, 1)\n",
    "        bwd_input = tf.expand_dims([self.tokenizer.word_index['<pad>']] * batch_size, 1)\n",
    "        hidden_states = tf.zeros((batch_size, 1, self.units + self.units)) # concatenated fwd and bwd hidden states\n",
    "\n",
    "        for j in range(findings.shape[2]): # generate each word (each sentence has a fixed # of words)\n",
    "            print(\"j\", j)\n",
    "            predictions, fwd_hidden, _ = self.fwd_decoder(fwd_input, features, prev_sentence, fwd_hidden)\n",
    "            loss += self.loss_function(findings[:, i, j], predictions)\n",
    "            fwd_input = tf.expand_dims(findings[:, i, j], 1)\n",
    "\n",
    "            predictions, bwd_hidden, _ = self.bwd_decoder(bwd_input, features, prev_sentence, bwd_hidden)\n",
    "            loss += self.loss_function(findings[:, i, -(j+1)], predictions)\n",
    "            bwd_input = tf.expand_dims(findings[:, i, -(j+1)], 1)\n",
    "\n",
    "            # Concat the bwd anf fwd hidden states\n",
    "            # (batch_size, 1, units + units)\n",
    "            if not is_training_impressions is True:\n",
    "                hidden = tf.concat([tf.expand_dims(fwd_hidden, 1), tf.expand_dims(bwd_hidden, 1)], axis=-1)\n",
    "                if self.tensors_are_same(hidden_states, tf.zeros((batch_size, 1, self.units + self.units))) is True:\n",
    "                  hidden_states = hidden\n",
    "                else:\n",
    "                  hidden_states = tf.concat([hidden_states, hidden], axis = 1)\n",
    "\n",
    "        if not is_training_impressions is True:\n",
    "            prev_sentence, _ = self.sentence_encoder(hidden_states, features)\n",
    "            print(hidden_states.shape, prev_sentence.shape)\n",
    "        return loss, prev_sentence, fwd_hidden, bwd_hidden\n",
    "\n",
    "    def train_fn(self, batch_size, img_tensor, findings):\n",
    "        loss = 0\n",
    "        with tf.GradientTape() as tape:\n",
    "            features = self.image_encoder(img_tensor)\n",
    "            encoded_sentences = tf.zeros((batch_size, 1, self.units))\n",
    "            prev_sentence = tf.zeros((batch_size, self.units))\n",
    "            fwd_hidden = tf.zeros((batch_size, self.units))\n",
    "            bwd_hidden = tf.zeros((batch_size, self.units))\n",
    "            # Generate Findings\n",
    "            for i in range(int(findings.shape[1]/2)): # for each sentence in \"findings\" (each batch has a fixed # of sentences)\n",
    "                print(\"-------------------------------------i:\", i)\n",
    "                loss, prev_sentence, fwd_hidden, bwd_hidden = self.train_word_decoder(batch_size, loss, features, findings, i, \\\n",
    "                                                                                      prev_sentence, fwd_hidden, bwd_hidden)\n",
    "                if self.tensors_are_same(encoded_sentences, tf.zeros((batch_size, 1, self.units))) is True:\n",
    "                    encoded_sentences = tf.expand_dims(prev_sentence, 1)\n",
    "                else:\n",
    "                    encoded_sentences = tf.concat([encoded_sentences, tf.expand_dims(prev_sentence, 1)], axis = 1)\n",
    "\n",
    "            encoded_paragraph, _ = self.paragraph_encoder(encoded_sentences, features)\n",
    "\n",
    "            # Generate Impressions\n",
    "            prev_sentence = encoded_paragraph\n",
    "            fwd_hidden = tf.zeros((batch_size, self.units))\n",
    "            bwd_hidden = tf.zeros((batch_size, self.units))\n",
    "            for i in range(int(findings.shape[1]/2), findings.shape[1]): # for each sentence in \"impressions\" (each batch has a fixed # of sentences)\n",
    "                print(\"-------------------------------------i:\", i)\n",
    "                loss, _, fwd_hidden, bwd_hidden = self.train_word_decoder(batch_size, loss, features, findings, i, \\\n",
    "                                                                          prev_sentence, fwd_hidden, bwd_hidden)\n",
    "\n",
    "        # Outside of \"With tf.GradientTape()\"\n",
    "        variables = self.image_encoder.variables + self.sentence_encoder.variables + self.paragraph_encoder.variables + \\\n",
    "                    self.fwd_decoder.variables + self.bwd_decoder.variables\n",
    "\n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        return loss, gradients, variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "7e7pqBRANGtD",
    "outputId": "e4a61a5a-b206-4e2d-efea-ed0761ba40ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://openi.nlm.nih.gov/imgs/collections/NLMCXR_png.tgz\n",
      "1360822272/1360814128 [==============================] - 33s 0us/step\n",
      "Downloading data from https://openi.nlm.nih.gov/imgs/collections/NLMCXR_reports.tgz\n",
      "1114112/1112632 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "maybe_download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "YmEv-KKmOynv",
    "outputId": "f056f5b6-ebdb-49ce-9d2c-d38786aa0a91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reports available: 3955\n",
      "Number of reports selected: 3331\n",
      "Number of reports not having images (skipped): 104\n",
      "Number of reports with Impression section empty (skipped): 6\n",
      "Number of reports with Findings section empty: 514\n",
      "Total skipped: 624\n"
     ]
    }
   ],
   "source": [
    "all_findings, all_impressions, all_img_names, rids=extract_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "v7aQCm8YyvAc",
    "outputId": "17828f0c-d1c3-4855-a59f-0d9d45d7c2e0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> The  examination consists of frontal and lateral radiographs of the chest <end>',\n",
       " '<start>  Again seen is evidence of prior CABG <end>',\n",
       " '<start>  The cardiomediastinal contours are unchanged <end>',\n",
       " '<start>    right and  left pleural effusions <end>',\n",
       " '<start>  There is  right greater than left bibasilar atelectasis <end>',\n",
       " '<start>   B-lines seen at the lung bases <end>',\n",
       " '<start>  No consolidation or pneumothorax <end>']"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_findings[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "tziVHAVB2Zam",
    "outputId": "9006e59b-ee3e-473f-ff58-5397904f36ba"
   },
   "outputs": [
    {
     "data": {
      "image/png": "/content/CXR2813_IM-1239-1001.png",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image('/content/CXR2813_IM-1239-1001.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rrGwFl4A2ZY3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i-dh-Qp8Oyqg"
   },
   "outputs": [],
   "source": [
    "MAX_PARAGRAPH_LENGTH = 5 # Fixed max number of sentences per report. This value comes from preprocessing\n",
    "MAX_SENTENCE_LENGTH = 18 # Fixed max number of words per sentence. This value comes from preprocessing\n",
    "max_sentence_length=18\n",
    "max_paragraph_length=5\n",
    "tokenizer, findings_vector=transform_input(all_findings, all_impressions, max_paragraph_length, max_sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "RtTSLbJCPYcU",
    "outputId": "10b6d220-3a0b-4809-e085-52828ea99137"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.2.0-rc2\n"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "8UFwWdtoP-Cm",
    "outputId": "387a3194-52af-4f48-811d-8f4fc26da799"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "87916544/87910968 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "inception_model = init_inception_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y5j1m93zPYfB"
   },
   "outputs": [],
   "source": [
    "# Create training and validation sets using 80-20 split\n",
    "img_name_train, img_name_test, findings_train, findings_test = train_test_split(all_img_names, findings_vector, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yLNIJRXvQ1h-"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(tokenizer, embedding_dim=256, units=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2IQjx_y8Q1k7"
   },
   "outputs": [],
   "source": [
    "FEATURES_SHAPE = 2048\n",
    "ATTENTION_FEATURES_SHAPE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6N07_T7qQ1oB"
   },
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_png(img, channels=3)\n",
    "    img = tf.image.resize_images(img, (299, 299))\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    return img, image_path\n",
    "\n",
    "def map_func(img_name, findings):\n",
    "    img, img_path = load_image(img_name)\n",
    "    img = tf.expand_dims(img, 0)\n",
    "    img_tensor = inception_model(img)\n",
    "    img_tensor = tf.reshape(img_tensor,\n",
    "                            (-1, img_tensor.shape[3]))\n",
    "\n",
    "    return img_tensor, findings\n",
    "\n",
    "def _set_shapes(images, findings):\n",
    "    # Statically set tensors dimensions\n",
    "    #print(images.get_shape(), findings.get_shape())\n",
    "    images.set_shape(tf.TensorShape([ATTENTION_FEATURES_SHAPE, FEATURES_SHAPE]))\n",
    "    findings.set_shape(findings.get_shape().merge_with(\n",
    "            tf.TensorShape([MAX_PARAGRAPH_LENGTH + MAX_PARAGRAPH_LENGTH, MAX_SENTENCE_LENGTH])))\n",
    "    return images, findings\n",
    "\n",
    "def input_fn(params):\n",
    "    batch_size = params['batch_size']\n",
    "    _findings_train = np.asarray(findings_train)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((img_name_train, _findings_train))\n",
    "\n",
    "    dataset = dataset.map(lambda item1, item2: tf.py_func(\n",
    "                map_func, [item1, item2], [tf.float32, tf.int32]), num_parallel_calls=FLAGS.num_shards)\n",
    "\n",
    "    #dataset = dataset.map(map_func)\n",
    "    dataset = dataset.map(functools.partial(_set_shapes))\n",
    "\n",
    "    # shuffling and batching\n",
    "    dataset = dataset.shuffle(10000).repeat()\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/contrib/data/batch_and_drop_remainder\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    dataset = dataset.prefetch(1)\n",
    "    # print(\"Dataset type:\", dataset.output_shapes, dataset.output_types)\n",
    "    return dataset\n",
    "\n",
    "def model_fn(features, labels, mode, params):\n",
    "\n",
    "    print(\"Model_Fn Shapes:\", features.shape, labels.shape)\n",
    "    print(\"Features:\", features)\n",
    "    batch_size = params['batch_size']\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.AdamOptimizer()\n",
    "        optimizer = tf.contrib.tpu.CrossShardOptimizer(optimizer)\n",
    "        loss, gradients, variables = trainer.train_fn(batch_size, features, labels)\n",
    "        train_op = optimizer.apply_gradients(zip(gradients, variables), tf.train.get_or_create_global_step())\n",
    "\n",
    "        return tf.contrib.tpu.TPUEstimatorSpec(mode=tf.estimator.ModeKeys.TRAIN,\n",
    "                                               loss = loss,\n",
    "                                               train_op = train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YBTit_3eNP12"
   },
   "outputs": [],
   "source": [
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.INFO)\n",
    "\n",
    "## https://stackoverflow.com/questions/55318626/module-tensorflow-has-no-attribute-logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nPp65OaGRqhH"
   },
   "outputs": [],
   "source": [
    "tpu_zone=None\n",
    "gcp_project=None\n",
    "tpu=None\n",
    "\n",
    "# tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\n",
    "#         tpu,\n",
    "#         zone=tpu_zone,\n",
    "#         project=gcp_project\n",
    "#     )\n",
    "\n",
    "# batch_size= 1024\n",
    "batch_size= 256\n",
    "\n",
    "\n",
    "# train_steps=1000\n",
    "train_steps=100\n",
    "eval_steps=0\n",
    "use_tpu=True\n",
    "enable_predict= True\n",
    "iterations_per_loop= 50\n",
    "# num_shards= 8 \n",
    "num_shards= 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "colab_type": "code",
    "id": "wxeNnbOkTLdj",
    "outputId": "b725c4f0-ddf4-4368-9996-865ef4615365"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-d244a9559f94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
     ]
    }
   ],
   "source": [
    "history=train(input_fn=input_fn, max_steps=train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZeU9Nr9FUTia"
   },
   "outputs": [],
   "source": [
    "trainer."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
